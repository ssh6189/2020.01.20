1. 기계학습과 딥러닝의 차이점은?

AI > 기계학습 > 딥러닝

임의의 세점이 있을때, 모델링하는법?

원초적으로 대충 선을 긋고, 그 선을 수식으로 표현 (기하학적 방식)

y1 = ax1 + b1
y2 = ax2 + b2
y3 = ax3 + b3
y4 = ax4 + b4
y5 = ax5 + b5

가장 근접한 a와 b를 찾는다.

이런 방법을 최소 자승법이라고 한다.



      o
    o     ㅁ  ㅁ
  o o     ㅁ  ㅁ
o         ㅁ
       ㅁ

두개를 구분하는 방법?

두개를 구분하는 직선을 찾는다.

y = ax + b

o와 ㅁ을 나눌 수 있는 가장 경계선, 이것의 중간 경계선을 구하는것이, 그게 SVM

y = ax + b

반복적인 최적화 방법을 통해, 미지수 a값과 b를 찾는것

공통점
주어진 데이터를 가지고, 모델링을 하여, 예측 시스템을 만드는것

차이점
딥러닝은 반복적인 최적화 방법 : 이것이 경사하강법이다.

경사하강법을 사용할때
장점 : 다층네트워크를 구축해 문제를 해결할 수 있다.

반복적인 최적화 -> 경사하강법 -> 다층 네트워크 -> 복잡
딥러닝은 이 다층 네트워크 깊이가 깊은것이다.



딥러닝 : 신경망 -> 가중치

2. 경사하강법 그림으로 표현하면?



3. 가중치는 (생물학적인) 신경망의 어디에 해당하는가?

y = sigma(w*x) + b

w(가중치) : 신경전달물질
동일 입력이다 하더라도, w에 따라, 다른 결과가 나온다.
w : 0이면, 아무리 입력을 해도 안된다는것

그래서, 인공신경망이라는 용어 탄생
y = sigma(w*x) > b

더한 값이, 임계치보다 높으면 활성화, 그렇지 않으면, 비활성화

w : 기울기 == 가중치 같은 의미
b : y절편 == 역치값 같은 의미

모양이 마음에 안들어서, 위치를 바꿈
(부호로 표현하고 싶어서)

y = sigma(w*x) - b
b : Bias라고 함



수학자들이 여기서, 또 변형
b를 없애려고함

CNN에서는 b를 안 쓰는 경우도 많다.
4. 어떻게 다층으로 복잡한 문제를 풀 수 있는가?

5. 다층네트워크를 학습시키기가 어려운 이유는?

6. 활성화는 비선형이어야 하는가?

7. Sigmoid 함수의 장점은?

8. y = wx + b에서 b는 왜 필요한가?

만약,

다음과 같은 그림이 있다.
        o
     o  o
     o
   o
o


y = wx로만 표현한다는것, 즉, 원점을 지난다는것, 제약조건이 심하다.

y = wx만으로, 적절하게 찾기?
원점을 바꾸면 된다.

9. 딥러닝의 핵심 돌파구는?

10. 왜 cnn을 특징 추출과 분류기 결합으로 보는가?



수업

신경망의 기본 가정이다.
h = wx + b

행렬로 바꾸어서 푼다.

역행렬을 이용한 최적해 구하기

w + b = 2
2w + b = 4.4
3w + b = 6.4

[1 1              2
2 1 =  [ w  = 4.4
3 1]      b ]   6.4

이 방법을 쓰면, 경사하강법을 이용하지 않아도, 잘나온다.


----------------------------------------------------------------------
어셈블리 프로그래머가 몸값 높다.

GPU, CPU는 하는 일이 다르다.

모나리자를 그리는 레오나르도 다빈치 로봇????
CPU와 GPU는 발상 자체가 다르다.
GPU는 단순한 연산만, 단순 반복이 많을시 장점이 많다.
CPU는 많은 제어 등을 할 수 있게 갖추어졌다.
----------------------------------------------------------------------
신경망은 다층으로 구성 가능하다.

경사하강법
사람은 바로 최소값이 어딘지를 아나, 컴퓨터는 그렇게 똑똑하지 못하다.
기울기의 반대방향만큼 조금씩 움직인다. 즉, 기울기가 양수면, 음의 방향으로, 음수면, 양의 방향으로 이동한다.
전체 함수를 알 필요없이, 지금지점과 다음지점의 값만 알면 된다.

gpu할때는, batchsize를 보통 작게 잡아준다. 메모리가 작다.
전체 데이터를 한번 다 돌은것을 epoch
즉, batchsize가 크면 클수록, 전체적인것을 수렴해서, 블러링(평균효과가 난다.)
batchsize 값이 크면 클수록, 느려진다.
batchsize 값이 작으면 작을수록, 빨라진다.
샘플수가 1000개일때, batchsize가 100이다. 그럼, 이렇게 10번을 해야 1epoch가 돌게 된다.
전체 데이터를 가지고 평균적으로 해서, 최소값까지 오나, 조금씩조금씩 해도, 언젠가는 도달한다.
그래서 확률적 경사하강법

층이 깊어질수록, 앞에는 학습이 잘되는데, 뒤에가 학습이 잘 안된다.
칠판하고 가까운 학생들은 학습이 잘 되는데, 먼곳 학생들은 학습이 잘 안된다.

model = Sequential()

model.add(Dense(10, input_dim=2))
model.add(Activation('sigmoid'))

model.add(Dense(10))
model.add(Activation('sigmoid'))

model.add(Dense(10))
model.add(Activation('sigmoid'))

model.add(Dense(10))
model.add(Activation('sigmoid'))

model.add(Dense(10))
model.add(Activation('sigmoid'))

model.add(Dense(5))
model.add(Activation('sigmoid'))


model.add(Dense(1))
model.add(Activation('sigmoid'))

model.summary()

윗부분이 잘 안되고, 아랫부분이 잘된다.
타겟값은 아랫부분밖에 없다.
그래서, 윗부분은 학습을 시킬 수가 없다.

아래가 앞이고, 위가 뒤다.
